{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r square = (yi - yi^)^2/(yi - y_mean)^2\n",
    "\n",
    "R¬≤, or R-Squared, is a metric that measures how well a regression model fits the data. To put it simply, it tells us what proportion of the variation in the actual values can be explained by the model's predictions.\n",
    "\n",
    "R¬≤ of a model is 0.85, it means 85% of the variance in sales is captured by the model, while the remaining 15% might be due to randomness or factors outside the model's scope.\n",
    "\n",
    "R-Squared, measures how much of the variation in the dependent variable (actual values) is explained by the model's predictions, and how much remains unexplained (randomness).\n",
    "\n",
    "R¬≤ is high, it means most of the variability in the data is being accounted for by the model, leaving only a small portion to randomness‚Äîlike unexpected spikes in ice cream sales during festivals. On the other hand, a lower R¬≤ indicates that the model struggles to explain the variation, leaving much of it as unexplained randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Define adjusted R-squared and explain how it differs from the regular R-squared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accountability for Variables:**\n",
    "\n",
    "R-squared always increases or stays the same when more predictors are added to the model, even if those predictors are not significant.\n",
    "\n",
    "Adjusted R-squared penalizes the model for including irrelevant variables by adjusting based on the number of predictors and the sample size.\n",
    "\n",
    "\n",
    "AR square helps in features selection in such a way that , when a new feature added is importtant for prediction iAR square increases if the feature added is not important with ref to the value to predicted AR square decreases . But R2 increases or remains same when the features are added .\n",
    "\n",
    "\n",
    "AR square = 1 - (1-R2)*(N-1)/N-P-1\n",
    "\n",
    "N-Number of data points\n",
    "P-Number of independent features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "It is appropriate to AR square when its about selecting new fetaures for predicting the target feature . Since AR square increases when new feature added is important and decreases when new feature added is not important allows the data scientists to select most appropriate feature for predicting the outcome .\n",
    "\n",
    "\n",
    "Adjusted R-squared is more appropriate when comparing models with a different number of independent variables. Unlike R-squared, which always increases when new predictors are added (even if they are irrelevant), adjusted R-squared accounts for the number of predictors and only increases if the new predictor improves the model significantly. If the new feature does not contribute meaningful information, adjusted R-squared decreases.\n",
    "\n",
    "\n",
    "Useful for :\n",
    "\n",
    "1. Feature selection\n",
    "2. Model evaluation - It penalize the when unecessary features are added as predictors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "\n",
    "\n",
    "RMSE , MSE and MAE are the errors between actual and predicted values . \n",
    "\n",
    "RMSE, MSE, and MAE in Regression Analysis\n",
    "In regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are commonly used metrics to evaluate the performance of a model by measuring the error between the actual and predicted values.\n",
    "\n",
    "1. Mean Absolute Error (MAE)\n",
    "Interpretation:\n",
    "MAE represents the average absolute difference between actual (\n",
    "ùë¶\n",
    "ùëñ\n",
    "y \n",
    "i\n",
    "‚Äã\n",
    " ) and predicted (\n",
    "ùë¶\n",
    "^\n",
    "ùëñ\n",
    "y\n",
    "^\n",
    "‚Äã\n",
    "  \n",
    "i\n",
    "‚Äã\n",
    " ) values.\n",
    "\n",
    "Pros: Easy to interpret, as it represents the error in actual units.\n",
    "\n",
    "Cons: Does not penalize large errors more than small ones.\n",
    "\n",
    "2. Mean Squared Error (MSE)\n",
    "\n",
    "MSE calculates the average of the squared differences between actual and predicted values.\n",
    "\n",
    "Pros: Gives more weight to larger errors, making it useful when large deviations need to be penalized.\n",
    "\n",
    "Cons: Not in the same units as the target variable (due to squaring).\n",
    "\n",
    "3. Root Mean Squared Error (RMSE)\n",
    "\n",
    "Interpretation:\n",
    "RMSE is simply the square root of MSE, bringing the error back to the same unit as the target variable.\n",
    "\n",
    "Pros: More interpretable than MSE since it's in the same units as the dependent variable.\n",
    "\n",
    "Cons: Still penalizes larger errors more than smaller ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "\n",
    "\n",
    "\n",
    "**MSE**:\n",
    "Advantage : \n",
    "1. Penalizes the large errors\n",
    "2. Differential at each point\n",
    "3. There exist only one global minima\n",
    "\n",
    "Disadvantage : \n",
    "\n",
    "1. Not robust to outliers\n",
    "2. Error is in squared format\n",
    "\n",
    "**RMSE**\n",
    "\n",
    "Advantage :\n",
    "1. Here error is in the same for as that of the feature\n",
    "2. This robust to outliers\n",
    "\n",
    "\n",
    "Disadvantage :\n",
    "1. It do not penalize/amlipfy large errors \n",
    "\n",
    "**MAE**\n",
    "\n",
    "Advantage :\n",
    "1. Robust to outliers \n",
    "\n",
    "Disadvantage :\n",
    "1. Convergence take time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "\n",
    "\n",
    "Lasso regularization also known as L1 Regularization is a feature selection technique used in Regression problem . In L1 regulariuzation sum of absolute value of slope multiplied by lambda is added to the cost function where the cost finction looks like :\n",
    "\n",
    "Cost function Error + lambda * (sum absolute value of slopes )\n",
    "\n",
    "y= m1x1 +m2x2 +m3x3 + c\n",
    "\n",
    "y=0.52x1 + 0.72x2 + 0.12x3 + 20 \n",
    "\n",
    "here feature x3 is less corr with the output y hence such a feature can be dropped . To drop the fetaure in cost function lambda values hould be selected in such a way that the m3 i.e solpe of the least corr feature becomes zero and add that combination in error in cost function.\n",
    "\n",
    "Lambda values are chosen in such a way that  the value of the param (m3) becomes zero and the fetire can be dropped.\n",
    "\n",
    "L1 Regularization:\n",
    "\n",
    "y=0.52x1 + 0.72x2 + 20\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Lasso Regularization (L1 Regularization)\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization, also known as L1 regularization, is a technique used in linear regression to prevent overfitting and perform feature selection by adding a penalty term to the cost function.\n",
    "\n",
    "Effect of Lasso Regularization\n",
    "Lasso tends to shrink some coefficients to exactly zero, effectively eliminating less important features from the model. This makes it a powerful tool for feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "\n",
    "\n",
    "Ovefitting in ML model is reduced using a concept called ridge regression . IN this process product of rigde parameter and sum of slope square is added to the MSE(cost function) so that the cost functionn does not becomes zero for training data and model does not overfit.\n",
    "\n",
    "Cost function = MSE + Lambda *(sum of slope square)\n",
    "\n",
    "Lambda value should be choosen in such a way that the cost function is not zero yet minimum , this avoids overfitting(High acc on traiing data and low acc on testting data) also help to keep the cost function low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "\n",
    "When to Avoid Regularized Linear Models?\n",
    " When the relationship between variables is highly non-linear.\n",
    " When there are complex interactions that linear models cannot capture.\n",
    " When working with small datasets, where regularization may remove too much information.\n",
    " When feature interpretability is a priority.\n",
    " \n",
    " \n",
    "\n",
    "**Feature Dependence & Collinearity Issues**\n",
    "\n",
    "Lasso regression (L1) can arbitrarily select one feature from a group of highly correlated features and shrink others to zero.\n",
    "\n",
    "Ridge regression (L2) does not eliminate features but still does not resolve collinearity issues perfectly.\n",
    "\n",
    "Alternative: Use Elastic Net, which combines L1 and L2 penalties to handle collinearity more effectively.\n",
    "\n",
    "**Choice of Regularization Parameter** \n",
    "\n",
    "If Œª is too large, it over-penalizes the model, leading to underfitting.\n",
    "\n",
    "If ùúÜ is too small, the effect of regularization is negligible, leading to overfitting.\n",
    "\n",
    "Alternative: Use techniques like grid search or Bayesian optimization to find the best ùúÜ\n",
    "\n",
    "\n",
    "**Assumption of Linearity**\n",
    "\n",
    "Regularized models assume a linear relationship between independent variables and the target.\n",
    "\n",
    "If the true relationship is non-linear, these models may fail to capture patterns in the data.\n",
    "\n",
    "Alternative: Use non-linear models like decision trees, random forests, or neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "Comparing Model A (RMSE = 10) and Model B (MAE = 8)\n",
    "RMSE (Root Mean Squared Error) penalizes larger errors more than smaller ones. It is useful when large errors should be heavily penalized.\n",
    "\n",
    "\n",
    "MAE (Mean Absolute Error) treats all errors equally, making it more robust to outliers.\n",
    "\n",
    "\n",
    "Since the models are evaluated using different metrics, a direct comparison is not straightforward. However, let‚Äôs consider different scenarios:\n",
    "If the dataset has outliers\n",
    "\n",
    "\n",
    "RMSE magnifies large errors, so Model A‚Äôs RMSE of 10 could indicate the presence of a few large errors.\n",
    "\n",
    "\n",
    "MAE (Model B = 8) suggests a smaller average absolute error, meaning it might be more robust to outliers.\n",
    "\n",
    "\n",
    "Conclusion: Model B may be the better choice if outliers are present.\n",
    "\n",
    "\n",
    "If minimizing large errors is crucial\n",
    "\n",
    "\n",
    "If large deviations are unacceptable (e.g., medical predictions, financial forecasting), RMSE is a better metric.\n",
    "\n",
    "\n",
    "Model A has a higher RMSE (10), indicating it may have some large errors.\n",
    "\n",
    "\n",
    "Conclusion: Model B might still be preferable if lower absolute error is more important.\n",
    "\n",
    "\n",
    "If models had the same evaluation metric\n",
    "\n",
    "\n",
    "If both were evaluated using RMSE or MAE, a direct comparison would be possible.\n",
    "\n",
    "\n",
    "Conclusion: Since RMSE and MAE measure errors differently, choosing based on these values alone is not ideal.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
