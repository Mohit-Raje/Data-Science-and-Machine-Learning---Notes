{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ee1275-85f7-4365-869f-d4fce0239348",
   "metadata": {},
   "source": [
    "### Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
    "\n",
    "\n",
    "Ridge Regression : Also known as L2 regularization is techinque that reduces/solves the problem of overfitting of model by adding the product of lambda(sum slope square) to cost function.\n",
    "\n",
    "Cost function = error + lambda(sum slope square) \n",
    "\n",
    "This ensures that the cost function would never become zero yet the value of lambda (Ridge factor) should be choosen in such a way that the cost function is minimum\n",
    "\n",
    "Ridge regression should be use when all the columns in the dataset are relevant with respect to the output feature.\n",
    "\n",
    "\n",
    "Ridge Regression vs. Ordinary Least Squares (OLS) Regression\n",
    "1. What is Ridge Regression?\n",
    "Ridge Regression (also called L2 regularization) is a type of linear regression that adds a penalty to the cost function to reduce overfitting. It is particularly useful when there is multicollinearity (high correlation between independent variables) in the dataset.\n",
    "\n",
    "2. Ordinary Least Squares (OLS) Regression\n",
    "OLS regression is the standard form of linear regression. It finds the best-fit line by minimizing the sum of squared residuals (errors):\n",
    "J(W)=âˆ‘(yiâˆ’y^i)2J(W) = \\sum (y_i - \\hat{y}_i)^2J(W)=âˆ‘(yiâ€‹âˆ’y^â€‹iâ€‹)2\n",
    "where:\n",
    "yiy_iyiâ€‹ = actual values\n",
    "\n",
    "\n",
    "y^i\\hat{y}_iy^â€‹iâ€‹ = predicted values\n",
    "\n",
    "\n",
    "WWW = regression coefficients (weights)\n",
    "\n",
    "\n",
    "Limitations of OLS:\n",
    " ðŸš« Overfitting: When there are too many features or multicollinearity, OLS may fit the training data too well but perform poorly on new data.\n",
    " ðŸš« Large Coefficients: OLS does not restrict coefficient sizes, which can lead to unstable models.\n",
    "\n",
    "3. How Ridge Regression Fixes OLS Issues\n",
    "Ridge regression modifies the OLS cost function by adding an L2 penalty term:\n",
    "J(W)=âˆ‘(yiâˆ’y^i)2+Î»âˆ‘Wi2J(W) = \\sum (y_i - \\hat{y}_i)^2 + \\lambda \\sum W_i^2J(W)=âˆ‘(yiâ€‹âˆ’y^â€‹iâ€‹)2+Î»âˆ‘Wi2â€‹\n",
    "where:\n",
    "Î»\\lambdaÎ» = regularization parameter (controls penalty strength).\n",
    "\n",
    "\n",
    "âˆ‘Wi2\\sum W_i^2âˆ‘Wi2â€‹ = sum of squared regression coefficients.\n",
    "\n",
    "\n",
    "Key Effects of Ridge Regression:\n",
    "âœ… Shrinks Coefficients: Reduces large values of WWW to prevent overfitting.\n",
    " âœ… Handles Multicollinearity: Distributes weight among correlated features instead of choosing one arbitrarily (like Lasso).\n",
    " âœ… Prevents Overfitting: Reduces variance by controlling coefficient sizes.\n",
    "\n",
    "\n",
    "5. When to Use Ridge Regression?\n",
    "âœ… When your dataset has highly correlated features (multicollinearity).\n",
    " âœ… When you have many features and want to reduce overfitting.\n",
    " âœ… When you need better generalization to new data.\n",
    " ðŸš« Do not use Ridge if you need automatic feature selection (Lasso is better for that).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38c9c70-12da-4b7c-a931-8b5d83f227d7",
   "metadata": {},
   "source": [
    "## Q2. What are the assumptions of Ridge Regression?\n",
    "\n",
    "**Assumption**:\n",
    "\n",
    "1. The features in the dataset are important and the purpose is to resolve the problem of overfitting .\n",
    "2. Linear relationship exists between the independent and dependent\n",
    "3. No Extreme Outliers\n",
    "\n",
    "        a. Ridge Regression is sensitive to outliers because it minimizes squared errors.\n",
    "        b. Large errors get squared and dominate the cost function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63192a84-bd5d-4b7f-b06f-1194624f9e01",
   "metadata": {},
   "source": [
    "## Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n",
    "\n",
    "1. Cross validation\n",
    "2. Data char\n",
    "3. GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f9e8a-ed0b-476f-9960-164d48f1c3e0",
   "metadata": {},
   "source": [
    "## Q4. Can Ridge Regression be used for feature selection? If yes, how?\n",
    "\n",
    "No ridge cannot be used for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f0513-e155-498e-9b47-a10572b4f0e6",
   "metadata": {},
   "source": [
    "## Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n",
    "\n",
    "1. Reduces overfitting\n",
    "2. Stabilizes the coefficient values \n",
    "3. Do not let one feature dominate other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf1d779-834d-4105-99a6-1f6370ab91fc",
   "metadata": {},
   "source": [
    "## Q6.Can Ridge Regression handle both categorical and continuous independent variables?\n",
    "\n",
    "1. Handling Continuous Independent Variables\n",
    "\n",
    "âœ… Ridge Regression works naturally with continuous variables.\n",
    "\n",
    "âœ… Example: If you have a dataset with age, salary, and temperature, Ridge can directly process these features.\n",
    "\n",
    "âœ… However, continuous variables should be standardized (e.g., using StandardScaler) for better performance.\n",
    "\n",
    "2. Handling Categorical Independent Variables\n",
    "\n",
    "ðŸš« Ridge does NOT work with raw categorical data (text or labels).\n",
    "\n",
    "âœ… Solution: Convert categorical variables into numerical form using encoding techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325e8305-00e7-415d-94f4-88128e40de9e",
   "metadata": {},
   "source": [
    "## Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d58c4e-6b8b-42b6-badd-be7e90841dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
