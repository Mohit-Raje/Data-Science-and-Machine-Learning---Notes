{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcccd4bc-8eeb-4288-a807-52c1e1f7c323",
   "metadata": {},
   "source": [
    "## Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "\n",
    "Euclidean distance measures the straight-line (diagonal) distance between two points.\n",
    "Manhattan distance measures the sum of absolute differences, like moving in a grid (no diagonals).\n",
    "\n",
    "üîç How it affects KNN performance:\n",
    "\n",
    "Scenario\t                        Preferred Distance Metric\t                               Why?\n",
    "Low-dimensional, continuous data\tEuclidean\t                                 Captures true geometric closeness\n",
    "High-dimensional or sparse data\t    Manhattan\t                            More stable, avoids distance flattening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7288622-b5b2-4bec-8b0c-db1f0c7b46a1",
   "metadata": {},
   "source": [
    "## Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?\n",
    "\n",
    "tO FIND THE OPTIMAL VALUE OF k Grid search CV OR Randomizedsearch cv can be used which is a part of hyperparameter tuning or else domian expertise as per the problem statetment\n",
    "\n",
    "Lower k values may lead to overfitting, while very high k values may cause underfitting. The goal is to find a k that balances bias and variance.\n",
    "\n",
    "If k is too small, like 1 or 2, the model just looks at very few neighbors ‚Äî it might get confused by noise and make wrong predictions. This is called overfitting ‚Äî the model is too focused on small details.\n",
    "\n",
    "If k is too large, like 50 or 100, the model averages over too many neighbors ‚Äî it becomes too general and might miss important patterns. This is called underfitting ‚Äî the model is too simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72012c9-0a79-4cc5-a567-01c75eea0a80",
   "metadata": {},
   "source": [
    "## Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n",
    "\n",
    "If the dataset is low dimensional or has continuous features or if it is a small dataset - Euclidean is preferable a;gorithm \n",
    "\n",
    "If the data is higher dimesnional , has sparse values(grid like)  or if the there are more feature - Manhattan is preferable\n",
    "\n",
    "üìå When to use Euclidean distance:\n",
    "Data is low-dimensional\n",
    "\n",
    "Features are continuous\n",
    "\n",
    "Dataset is small and dense\n",
    "\n",
    "Relationships between points are diagonal or curved\n",
    "\n",
    "üß† Example: Predicting prices using features like area, age, and number of rooms.\n",
    "\n",
    "üìå When to use Manhattan distance:\n",
    "Data is high-dimensional\n",
    "\n",
    "Data is sparse (many zeros or grid-like patterns)\n",
    "\n",
    "Movement is axis-aligned\n",
    "\n",
    "You want better robustness in high dimensions\n",
    "\n",
    "üß† Example: Text classification using bag-of-words vectors (many features, most are zero).\n",
    "\n",
    "\n",
    "Euclidean is for smooth, continuous space ‚Äî Manhattan is better when data is structured like a grid or high-dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ac466-d572-48b2-ac02-f0a86824ea3b",
   "metadata": {},
   "source": [
    "## Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n",
    "\n",
    "The most importnat parameters in KNN classifier and regressor are : \n",
    "\n",
    "1. n_neighbours(k) - This value of K , preferrably its decided using hyperparametr tuning or using domain expretise. Lower value of k - overfitting , higher value of k undrfitting\n",
    "\n",
    "2. Algorithm - brutte - This basic KNN where distance between the test point and all the point is calculated\n",
    "               KD tree - Optimized binary tree but backtracking involved which increase the time complexity\n",
    "               Ball tree - This is the most optimized version of KNN where no backtracking is involvde also minimal distance calculation unlike the bruteeforce \n",
    "               auto : select the best algorithm as per dataste\n",
    "\n",
    "3. P - 2 - Euclidean - when the space is smooth and values are continuos , dataset is lower dimensional\n",
    "       1 - Manhattan - when the space is grid , and highefr dimensional dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274ff03-f564-4b8e-b57b-b0e7cf5f2984",
   "metadata": {},
   "source": [
    "## Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?\n",
    "\n",
    "A larger training set improves accuracy in KNN, but slows down prediction ‚Äî so techniques like sampling and dimensionality reduction help balance performance and efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ea3c7-423a-43ad-a2c6-257a4bafa108",
   "metadata": {},
   "source": [
    "## Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?\n",
    "\n",
    "Potential drawbacks of KNN are if the number feature are its cause curse of dimensionality and evetually perfomance of teh KNN is degaraged as number of features becomes more distacne becomes a inappropriante to do predicts and KNN's core becomes worthless. To overcome this PCA can be used dimensionality reduction technique\n",
    "\n",
    "If the number of datapaoints are to any the accuracy amy increase but more time is consumed in the overall process to overcome this stratified sampling is used .\n",
    "\n",
    "To overcome the bruteeforce time consuming approch KD tree and Ball tree are used \n",
    "\n",
    "Value of K should be choose apppropriately to avoid overfitting or underfitting and distance metric should chosen as per the dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
